{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CLASSIFICATORE LINEARE SVM CON INPUT DI N-GRAMMI\n",
    "\n",
    "Classificatore basato su  SVM che prende in input una matrice di features basata su n-grammi di 3 tipi:\n",
    "1. Caratteri\n",
    "2. Token\n",
    "3. Part of Speech\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dei dati\n",
    "\n",
    "Dobbiamo ottenere un vettore di features, basato sulle occorrenze di n-grammi all'interno delle frasi, per ciascun post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creiamo un dataframe che abbia 4 colonne, id (identificativo frase), word, lemma\n",
    "def create_set(type):\n",
    "    annotated_posts = []\n",
    "    for doc in os.listdir(\"../data/UD_annotation\"):\n",
    "        if type in doc: \n",
    "            doc_tokens = []\n",
    "            doc_path = \"../data/UD_annotation/\" + doc\n",
    "            for line in open(doc_path, \"r\", encoding=\"utf-8\"):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                if splitted[0].isdigit() and \"-\" not in splitted[0]:\n",
    "                    word=splitted[1]\n",
    "                    lemma=splitted[2]\n",
    "                    pos=splitted[3]\n",
    "                    new_token = {'word': word, 'lemma': lemma, 'pos': pos}\n",
    "                    doc_tokens.append(new_token)\n",
    "            annotated_posts.append(doc_tokens)\n",
    "    return annotated_posts\n",
    "\n",
    "annotation_tr = create_set(\"training\")\n",
    "annotation_ts = create_set(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Le', 'lemma': 'il', 'pos': 'DET'}, {'word': '5', 'lemma': '5', 'pos': 'NUM'}, {'word': 'sgradevoli', 'lemma': 'sgradevole', 'pos': 'ADJ'}, {'word': 'realtà', 'lemma': 'realtà', 'pos': 'NOUN'}, {'word': 'di', 'lemma': 'di', 'pos': 'ADP'}, {'word': 'cui', 'lemma': 'cui', 'pos': 'PRON'}, {'word': 'Berlusconi', 'lemma': 'Berlusconi', 'pos': 'PROPN'}, {'word': 'dovrebbe', 'lemma': 'dovere', 'pos': 'AUX'}, {'word': 'render', 'lemma': 'rendere', 'pos': 'VERB'}, {'word': 'si', 'lemma': 'si', 'pos': 'PRON'}, {'word': 'personalmente', 'lemma': 'personalmente', 'pos': 'ADV'}, {'word': 'conto', 'lemma': 'conto', 'pos': 'NOUN'}, {'word': '<URL>', 'lemma': '<URL>', 'pos': 'PROPN'}, {'word': 'Mario', 'lemma': 'Mario', 'pos': 'PROPN'}, {'word': 'Monti', 'lemma': 'Monti', 'pos': 'PROPN'}, {'word': 'non', 'lemma': 'non', 'pos': 'ADV'}, {'word': 'usa', 'lemma': 'usare', 'pos': 'VERB'}, {'word': 'mezzi', 'lemma': 'mezzo', 'pos': 'ADJ'}, {'word': 'termini', 'lemma': 'termine', 'pos': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "print(annotation_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 3977\n",
      "Test: 872\n"
     ]
    }
   ],
   "source": [
    "#contiamo il numero di post(sample) per training e test\n",
    "print(f'Training: {len(annotation_tr)}')\n",
    "print(f'Test: {len(annotation_ts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "{'word_2_L_e': 1, 'word_2_e_ ': 3, 'word_2_ _5': 1, 'word_2_5_ ': 1, 'word_2_ _s': 2, 'word_2_s_g': 1, 'word_2_g_r': 1, 'word_2_r_a': 1, 'word_2_a_d': 1, 'word_2_d_e': 2, 'word_2_e_v': 1, 'word_2_v_o': 1, 'word_2_o_l': 1, 'word_2_l_i': 1, 'word_2_i_ ': 7, 'word_2_ _r': 2, 'word_2_r_e': 3, 'word_2_e_a': 1, 'word_2_a_l': 2, 'word_2_l_t': 1, 'word_2_t_à': 1, 'word_2_à_ ': 1, 'word_2_ _d': 2, 'word_2_d_i': 1, 'word_2_ _c': 2, 'word_2_c_u': 1, 'word_2_u_i': 1, 'word_2_ _B': 1, 'word_2_B_e': 1, 'word_2_e_r': 4, 'word_2_r_l': 1, 'word_2_l_u': 1, 'word_2_u_s': 2, 'word_2_s_c': 1, 'word_2_c_o': 2, 'word_2_o_n': 5, 'word_2_n_i': 2, 'word_2_d_o': 1, 'word_2_o_v': 1, 'word_2_v_r': 1, 'word_2_e_b': 1, 'word_2_b_b': 1, 'word_2_b_e': 1, 'word_2_e_n': 2, 'word_2_n_d': 1, 'word_2_r_ ': 1, 'word_2_s_i': 1, 'word_2_ _p': 1, 'word_2_p_e': 1, 'word_2_r_s': 1, 'word_2_s_o': 1, 'word_2_n_a': 1, 'word_2_l_m': 1, 'word_2_m_e': 2, 'word_2_n_t': 3, 'word_2_t_e': 2, 'word_2_t_o': 1, 'word_2_o_ ': 2, 'word_2_ _<': 1, 'word_2_<_U': 1, 'word_2_U_R': 1, 'word_2_R_L': 1, 'word_2_L_>': 1, 'word_2_>_ ': 1, 'word_2_ _M': 2, 'word_2_M_a': 1, 'word_2_a_r': 1, 'word_2_r_i': 1, 'word_2_i_o': 1, 'word_2_M_o': 1, 'word_2_t_i': 1, 'word_2_ _n': 1, 'word_2_n_o': 1, 'word_2_n_ ': 1, 'word_2_ _u': 1, 'word_2_s_a': 1, 'word_2_a_ ': 1, 'word_2_ _m': 1, 'word_2_e_z': 1, 'word_2_z_z': 1, 'word_2_z_i': 1, 'word_2_ _t': 1, 'word_2_r_m': 1, 'word_2_m_i': 1, 'word_2_i_n': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_ngrams(post, ft_type, n, char=False):\n",
    "    all_ngrams = {}\n",
    "    allwords = []\n",
    "    if not char:\n",
    "        for word in post:\n",
    "            allwords.append(word[ft_type])\n",
    "        post_length = len(allwords)\n",
    "    else:\n",
    "        for word in post:\n",
    "            allwords.append(word['word'])\n",
    "        allwords = \" \".join(allwords)\n",
    "        post_length = len(allwords)-1\n",
    "    for i in range(len(allwords)-(n-1)):\n",
    "        new_ngram = allwords[i:i+n]\n",
    "        ngram_id = f'{ft_type}_{n}_'+'_'.join(new_ngram)\n",
    "        if ngram_id not in all_ngrams:\n",
    "            all_ngrams[ngram_id] = 1\n",
    "        else:\n",
    "            all_ngrams[ngram_id]+=1\n",
    "    #normalizzare in base alla post_length\n",
    "    print(post_length)\n",
    "    return all_ngrams\n",
    "        \n",
    "\n",
    "print(count_ngrams(annotation_tr[0], \"word\", 2, True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
