{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CLASSIFICATORE LINEARE SVM CON INPUT DI N-GRAMMI\n",
    "\n",
    "Classificatore basato su  SVM che prende in input una matrice di features basata su n-grammi di 3 tipi:\n",
    "1. Caratteri\n",
    "2. Token\n",
    "3. Part of Speech\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dei dati\n",
    "\n",
    "Dobbiamo ottenere un vettore di features, basato sulle occorrenze di n-grammi all'interno delle frasi, per ciascun post. Come prima cosa si crea si per training che per test un set formato da liste di dizionari. I dizionari riportano per ogni token 3 campi che corrispondono alla parola, la forma lemmatizzata e la parte del discorso. Infine contiamo all'interno dei post le occorrenze degli n-grammi. Una volta ottenuti tutti gli n-grammi possiamo creare la matrice di occorrenze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creiamo un dizionario che abbia 3 campi: word, lemma e part of speech(pos)\n",
    "def create_set(type):\n",
    "    annotated_posts = []\n",
    "    for doc in os.listdir(\"../data/UD_annotation\"):\n",
    "        if type in doc: \n",
    "            doc_tokens = []\n",
    "            doc_path = \"../data/UD_annotation/\" + doc\n",
    "            for line in open(doc_path, \"r\", encoding=\"utf-8\"):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                if splitted[0].isdigit() and \"-\" not in splitted[0]:\n",
    "                    word=splitted[1]\n",
    "                    lemma=splitted[2]\n",
    "                    pos=splitted[3]\n",
    "                    new_token = {'word': word, 'lemma': lemma, 'pos': pos}\n",
    "                    doc_tokens.append(new_token)\n",
    "            annotated_posts.append(doc_tokens)\n",
    "    return annotated_posts\n",
    "\n",
    "annotation_tr = create_set(\"training\")\n",
    "annotation_ts = create_set(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Le', 'lemma': 'il', 'pos': 'DET'}, {'word': '5', 'lemma': '5', 'pos': 'NUM'}, {'word': 'sgradevoli', 'lemma': 'sgradevole', 'pos': 'ADJ'}, {'word': 'realtà', 'lemma': 'realtà', 'pos': 'NOUN'}, {'word': 'di', 'lemma': 'di', 'pos': 'ADP'}, {'word': 'cui', 'lemma': 'cui', 'pos': 'PRON'}, {'word': 'Berlusconi', 'lemma': 'Berlusconi', 'pos': 'PROPN'}, {'word': 'dovrebbe', 'lemma': 'dovere', 'pos': 'AUX'}, {'word': 'render', 'lemma': 'rendere', 'pos': 'VERB'}, {'word': 'si', 'lemma': 'si', 'pos': 'PRON'}, {'word': 'personalmente', 'lemma': 'personalmente', 'pos': 'ADV'}, {'word': 'conto', 'lemma': 'conto', 'pos': 'NOUN'}, {'word': '<URL>', 'lemma': '<URL>', 'pos': 'PROPN'}, {'word': 'Mario', 'lemma': 'Mario', 'pos': 'PROPN'}, {'word': 'Monti', 'lemma': 'Monti', 'pos': 'PROPN'}, {'word': 'non', 'lemma': 'non', 'pos': 'ADV'}, {'word': 'usa', 'lemma': 'usare', 'pos': 'VERB'}, {'word': 'mezzi', 'lemma': 'mezzo', 'pos': 'ADJ'}, {'word': 'termini', 'lemma': 'termine', 'pos': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "print(annotation_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 3977\n",
      "Test: 872\n"
     ]
    }
   ],
   "source": [
    "#contiamo il numero di post(sample) per training e test\n",
    "print(f'Training: {len(annotation_tr)}')\n",
    "print(f'Test: {len(annotation_ts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_2_#la7_ma': 0.05555555555555555, 'word_2_ma_perche': 0.05555555555555555, \"word_2_perche_'\": 0.05555555555555555, \"word_2_'_Mario\": 0.05555555555555555, 'word_2_Mario_Monti': 0.05555555555555555, 'word_2_Monti_non': 0.05555555555555555, 'word_2_non_fa': 0.05555555555555555, 'word_2_fa_il': 0.05555555555555555, 'word_2_il_premier': 0.05555555555555555, 'word_2_premier_?': 0.05555555555555555, 'word_2_?_Che': 0.05555555555555555, 'word_2_Che_persona': 0.05555555555555555, 'word_2_persona_competente': 0.05555555555555555, 'word_2_competente_e': 0.05555555555555555, 'word_2_e_per': 0.05555555555555555, 'word_2_per_bene': 0.05555555555555555, 'word_2_bene_!': 0.05555555555555555}\n"
     ]
    }
   ],
   "source": [
    "#funzione che conta le occorrenze degli ngrammi all'interno di un post\n",
    "def count_ngrams(post, ft_type, n, char=False):\n",
    "    all_ngrams = {}\n",
    "    allwords = []\n",
    "    if not char:\n",
    "        for word in post:\n",
    "            allwords.append(word[ft_type])\n",
    "        post_length = len(allwords)\n",
    "    else:\n",
    "        for word in post:\n",
    "            allwords.append(word['word'])\n",
    "        allwords = \" \".join(allwords)\n",
    "        post_length = len(allwords)-1\n",
    "    for i in range(len(allwords)-(n-1)):\n",
    "        new_ngram = allwords[i:i+n]\n",
    "        if not char:\n",
    "            ngram_id = f'{ft_type}_{n}_'+'_'.join(new_ngram)\n",
    "        else:\n",
    "            ngram_id = f'char_{n}_'+'_'.join(new_ngram)\n",
    "        if ngram_id not in all_ngrams:\n",
    "            all_ngrams[ngram_id] = 1\n",
    "        else:\n",
    "            all_ngrams[ngram_id]+=1\n",
    "    #normalizzare in base alla post_length\n",
    "    for ngram in all_ngrams:\n",
    "        all_ngrams[ngram] = all_ngrams[ngram]/post_length\n",
    "    return all_ngrams\n",
    "\n",
    "print(count_ngrams(annotation_tr[2], \"word\", 2, False))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniamo per ogni tipo di informazione che vogliamo i relativi dizionari, così da avere un dizionario denso di informazioni. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_1_Mario': 0.043478260869565216, 'word_1_Monti': 0.043478260869565216, 'word_1_:': 0.043478260869565216, \"word_1_c'\": 0.043478260869565216, 'word_1_è': 0.043478260869565216, 'word_1_il': 0.043478260869565216, 'word_1_rischio...': 0.043478260869565216, 'word_1_di': 0.08695652173913043, 'word_1_trasformare': 0.043478260869565216, \"word_1_l'\": 0.08695652173913043, 'word_1_Italia': 0.043478260869565216, 'word_1_da': 0.043478260869565216, 'word_1_Stato': 0.08695652173913043, 'word_1_fondatore': 0.043478260869565216, 'word_1_in': 0.043478260869565216, 'word_1_affondatore': 0.043478260869565216, 'word_1_Unione': 0.043478260869565216, 'word_1_europea': 0.043478260869565216, 'word_1_!': 0.043478260869565216, 'word_1_<URL>': 0.043478260869565216, 'word_2_Mario_Monti': 0.043478260869565216, 'word_2_Monti_:': 0.043478260869565216, \"word_2_:_c'\": 0.043478260869565216, \"word_2_c'_è\": 0.043478260869565216, 'word_2_è_il': 0.043478260869565216, 'word_2_il_rischio...': 0.043478260869565216, 'word_2_rischio..._di': 0.043478260869565216, 'word_2_di_trasformare': 0.043478260869565216, \"word_2_trasformare_l'\": 0.043478260869565216, \"word_2_l'_Italia\": 0.043478260869565216, 'word_2_Italia_da': 0.043478260869565216, 'word_2_da_Stato': 0.043478260869565216, 'word_2_Stato_fondatore': 0.043478260869565216, 'word_2_fondatore_in': 0.043478260869565216, 'word_2_in_Stato': 0.043478260869565216, 'word_2_Stato_affondatore': 0.043478260869565216, 'word_2_affondatore_di': 0.043478260869565216, \"word_2_di_l'\": 0.043478260869565216, \"word_2_l'_Unione\": 0.043478260869565216, 'word_2_Unione_europea': 0.043478260869565216, 'word_2_europea_!': 0.043478260869565216, 'word_2_!_<URL>': 0.043478260869565216, 'char_1_M': 0.016, 'char_1_a': 0.096, 'char_1_r': 0.064, 'char_1_i': 0.08, 'char_1_o': 0.096, 'char_1_ ': 0.176, 'char_1_n': 0.048, 'char_1_t': 0.072, 'char_1_:': 0.008, 'char_1_c': 0.016, \"char_1_'\": 0.024, 'char_1_è': 0.008, 'char_1_l': 0.032, 'char_1_s': 0.016, 'char_1_h': 0.008, 'char_1_.': 0.024, 'char_1_d': 0.04, 'char_1_f': 0.032, 'char_1_m': 0.008, 'char_1_e': 0.048, 'char_1_I': 0.008, 'char_1_S': 0.016, 'char_1_U': 0.016, 'char_1_u': 0.008, 'char_1_p': 0.008, 'char_1_!': 0.008, 'char_1_<': 0.008, 'char_1_R': 0.008, 'char_1_L': 0.008, 'char_1_>': 0.008, 'char_2_M_a': 0.008, 'char_2_a_r': 0.016, 'char_2_r_i': 0.016, 'char_2_i_o': 0.024, 'char_2_o_ ': 0.024, 'char_2_ _M': 0.008, 'char_2_M_o': 0.008, 'char_2_o_n': 0.032, 'char_2_n_t': 0.008, 'char_2_t_i': 0.008, 'char_2_i_ ': 0.024, 'char_2_ _:': 0.008, 'char_2_:_ ': 0.008, 'char_2_ _c': 0.008, \"char_2_c_'\": 0.008, \"char_2_'_ \": 0.024, 'char_2_ _è': 0.008, 'char_2_è_ ': 0.008, 'char_2_ _i': 0.016, 'char_2_i_l': 0.008, 'char_2_l_ ': 0.008, 'char_2_ _r': 0.008, 'char_2_i_s': 0.008, 'char_2_s_c': 0.008, 'char_2_c_h': 0.008, 'char_2_h_i': 0.008, 'char_2_o_.': 0.008, 'char_2_._.': 0.016, 'char_2_._ ': 0.008, 'char_2_ _d': 0.024, 'char_2_d_i': 0.016, 'char_2_ _t': 0.008, 'char_2_t_r': 0.008, 'char_2_r_a': 0.008, 'char_2_a_s': 0.008, 'char_2_s_f': 0.008, 'char_2_f_o': 0.024, 'char_2_o_r': 0.024, 'char_2_r_m': 0.008, 'char_2_m_a': 0.008, 'char_2_r_e': 0.024, 'char_2_e_ ': 0.032, 'char_2_ _l': 0.016, \"char_2_l_'\": 0.016, 'char_2_ _I': 0.008, 'char_2_I_t': 0.008, 'char_2_t_a': 0.024, 'char_2_a_l': 0.008, 'char_2_l_i': 0.008, 'char_2_i_a': 0.008, 'char_2_a_ ': 0.024, 'char_2_d_a': 0.024, 'char_2_ _S': 0.016, 'char_2_S_t': 0.016, 'char_2_a_t': 0.032, 'char_2_t_o': 0.032, 'char_2_ _f': 0.008, 'char_2_n_d': 0.016, 'char_2_i_n': 0.008, 'char_2_n_ ': 0.008, 'char_2_ _a': 0.008, 'char_2_a_f': 0.008, 'char_2_f_f': 0.008, 'char_2_ _U': 0.008, 'char_2_U_n': 0.008, 'char_2_n_i': 0.008, 'char_2_n_e': 0.008, 'char_2_ _e': 0.008, 'char_2_e_u': 0.008, 'char_2_u_r': 0.008, 'char_2_r_o': 0.008, 'char_2_o_p': 0.008, 'char_2_p_e': 0.008, 'char_2_e_a': 0.008, 'char_2_ _!': 0.008, 'char_2_!_ ': 0.008, 'char_2_ _<': 0.008, 'char_2_<_U': 0.008, 'char_2_U_R': 0.008, 'char_2_R_L': 0.008, 'char_2_L_>': 0.008}\n"
     ]
    }
   ],
   "source": [
    "def get_features(dataset, info_types):\n",
    "    all_features = []\n",
    "    for post in dataset: \n",
    "        complete_dict = dict()\n",
    "        for info in info_types:\n",
    "            info_dict = count_ngrams(post, info[0], info[1], info[2])\n",
    "            complete_dict = complete_dict | info_dict\n",
    "        all_features.append(complete_dict)\n",
    "    return all_features\n",
    "\n",
    "infos = [['word', 1, False], ['word', 2, False], ['word', 1, True], ['word', 2, True]]\n",
    "\n",
    "tr_features = get_features(annotation_tr, infos)\n",
    "\n",
    "print(tr_features[1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ridurre la dimensionalità della nostra matrice, eliminiamo gli n-grammi che occorrono sotto a un numero soglia di documenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64468\n"
     ]
    }
   ],
   "source": [
    "def get_feature_list(feature_set):\n",
    "    feature_list = []\n",
    "    for post in feature_set:\n",
    "        for feature in post:\n",
    "            if feature not in feature_list:\n",
    "                feature_list.append(feature)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = get_feature_list(tr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_common(feature, feature_set, min_occurrences):\n",
    "    count = 0\n",
    "    for post in feature_set:\n",
    "        if feature in post:\n",
    "            count += 1\n",
    "    if count < min_occurrences:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "filtered_features = list(filter(lambda x: is_common(x, tr_features, 3), all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_feature(feature, features_tokeep):\n",
    "    if feature in features_tokeep:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def filter_dataset(dataset):\n",
    "    ##filtrare con pop() \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7534\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
