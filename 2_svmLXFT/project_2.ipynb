{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CLASSIFICATORE LINEARE SVM CON INPUT DI N-GRAMMI\n",
    "\n",
    "Classificatore basato su  SVM che prende in input una matrice di features basata su n-grammi di 3 tipi:\n",
    "1. Caratteri\n",
    "2. Token\n",
    "3. Part of Speech\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dei dati\n",
    "\n",
    "Dobbiamo ottenere un vettore di features, basato sulle occorrenze di n-grammi all'interno delle frasi, per ciascun post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creiamo un dataframe che abbia 4 colonne, id (identificativo frase), word, lemma\n",
    "def create_set(type):\n",
    "    annotated_posts = []\n",
    "    for doc in os.listdir(\"../data/UD_annotation\"):\n",
    "        if type in doc: \n",
    "            doc_tokens = []\n",
    "            doc_path = \"../data/UD_annotation/\" + doc\n",
    "            for line in open(doc_path, \"r\", encoding=\"utf-8\"):\n",
    "                splitted = line.strip().split(\"\\t\")\n",
    "                if splitted[0].isdigit() and \"-\" not in splitted[0]:\n",
    "                    word=splitted[1]\n",
    "                    lemma=splitted[2]\n",
    "                    pos=splitted[3]\n",
    "                    new_token = {'word': word, 'lemma': lemma, 'pos': pos}\n",
    "                    doc_tokens.append(new_token)\n",
    "            annotated_posts.append(doc_tokens)\n",
    "    return annotated_posts\n",
    "\n",
    "annotation_tr = create_set(\"training\")\n",
    "annotation_ts = create_set(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Le', 'lemma': 'il', 'pos': 'DET'}, {'word': '5', 'lemma': '5', 'pos': 'NUM'}, {'word': 'sgradevoli', 'lemma': 'sgradevole', 'pos': 'ADJ'}, {'word': 'realtà', 'lemma': 'realtà', 'pos': 'NOUN'}, {'word': 'di', 'lemma': 'di', 'pos': 'ADP'}, {'word': 'cui', 'lemma': 'cui', 'pos': 'PRON'}, {'word': 'Berlusconi', 'lemma': 'Berlusconi', 'pos': 'PROPN'}, {'word': 'dovrebbe', 'lemma': 'dovere', 'pos': 'AUX'}, {'word': 'render', 'lemma': 'rendere', 'pos': 'VERB'}, {'word': 'si', 'lemma': 'si', 'pos': 'PRON'}, {'word': 'personalmente', 'lemma': 'personalmente', 'pos': 'ADV'}, {'word': 'conto', 'lemma': 'conto', 'pos': 'NOUN'}, {'word': '<URL>', 'lemma': '<URL>', 'pos': 'PROPN'}, {'word': 'Mario', 'lemma': 'Mario', 'pos': 'PROPN'}, {'word': 'Monti', 'lemma': 'Monti', 'pos': 'PROPN'}, {'word': 'non', 'lemma': 'non', 'pos': 'ADV'}, {'word': 'usa', 'lemma': 'usare', 'pos': 'VERB'}, {'word': 'mezzi', 'lemma': 'mezzo', 'pos': 'ADJ'}, {'word': 'termini', 'lemma': 'termine', 'pos': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "print(annotation_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 3977\n",
      "Test: 872\n"
     ]
    }
   ],
   "source": [
    "#contiamo il numero di post(sample) per training e test\n",
    "print(f'Training: {len(annotation_tr)}')\n",
    "print(f'Test: {len(annotation_ts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_2_#la7_ma': 1, 'word_2_ma_perche': 1, \"word_2_perche_'\": 1, \"word_2_'_Mario\": 1, 'word_2_Mario_Monti': 1, 'word_2_Monti_non': 1, 'word_2_non_fa': 1, 'word_2_fa_il': 1, 'word_2_il_premier': 1, 'word_2_premier_?': 1, 'word_2_?_Che': 1, 'word_2_Che_persona': 1, 'word_2_persona_competente': 1, 'word_2_competente_e': 1, 'word_2_e_per': 1, 'word_2_per_bene': 1, 'word_2_bene_!': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_ngrams(post, ft_type,n):\n",
    "    all_ngrams = {}\n",
    "    allwords = []\n",
    "    for word in post:\n",
    "        allwords.append(word[ft_type])\n",
    "    for i in range(len(allwords)-(n-1)):\n",
    "        new_ngram = allwords[i:i+n]\n",
    "        ngram_id = f'{ft_type}_{n}_'+'_'.join(new_ngram)\n",
    "        if ngram_id not in all_ngrams:\n",
    "            all_ngrams[ngram_id] = 1\n",
    "        else:\n",
    "            all_ngrams[ngram_id]+=1\n",
    "    return all_ngrams\n",
    "        \n",
    "\n",
    "print(count_ngrams(annotation_tr[2], \"word\", 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
