{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATORE SVM LINEARE CON INPUT DI WORD EMBEDDINGS\n",
    "\n",
    "Classificatore basato su svm lineare che prende in input i word embeddings di parole italiane, più specificamente dei word embedding sviluppati per EvalIta 2018 dall'Italian NLP Lab. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento Word Embeddings\n",
    "\n",
    "Il file txt dei word embedding è stato ottenuto attraverso l'elaborazione di un file sqlite. Per ogni riga (la quale rappresenta una parola) all'interno del file, si crea un record in un dizionario con chiave parola e come valore il vattore che rappresenta il word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per il caricamento degli embedding itwac\n",
    "def load_embeddings():\n",
    "    embeddings_dict = dict()\n",
    "    for line in open('../data/embeddings/itwac32.txt', encoding='utf-8'):\n",
    "        splitted = line.strip().split('\\t')\n",
    "        word = splitted[0]\n",
    "        embedding = splitted[1:]\n",
    "        embedding = [float(val) for val in embedding]\n",
    "        embeddings_dict[word] = np.asarray(embedding)\n",
    "    return embeddings_dict\n",
    "\n",
    "embeddings = load_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02069134,  0.09119736,  0.25785723, -0.23561105, -0.28197852,\n",
       "       -0.13193955, -0.13197723, -0.05229089, -0.28881341,  0.06564969,\n",
       "       -0.30802506,  0.11779311, -0.03571652, -0.08748714, -0.24729131,\n",
       "        0.2577146 ,  0.11925782, -0.27795964,  0.20498367,  0.08414506,\n",
       "        0.08175091, -0.05181665, -0.34403449, -0.05261306,  0.08858071,\n",
       "       -0.09748928,  0.12879393,  0.04387102, -0.04690454,  0.08181785,\n",
       "        0.321078  ,  0.01658573])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['amico']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizzazione del testo \n",
    "\n",
    "Le parole all'interno dei post devono essere normalizzate in questo modo:\n",
    "\n",
    "Numeri:\n",
    "- interi tra 0 e 2100 possono essere mantenuti così come sono\n",
    "- i numeri interi maggiori di 2100 diventano una stringa specifica con un numero che rappresenta il numero di cifre\n",
    "- se non si tratta di numeri interi, si convertono le cifre all'interno della stringa in questa sequenze @Dg\n",
    "\n",
    "Parole:\n",
    "- le parole che iniziano con una lettera maiuscola devono avere prima parola maiuscola e le altre minuscole\n",
    "- le parole che iniziano con una lettera minuscola devono avere tutti i caratteri minuscolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione che trasforma le cifre all'interno di un token\n",
    "def digit_norm(word):\n",
    "    try:\n",
    "        val = int(word)\n",
    "    except:\n",
    "        normalized = re.sub('\\d', '@Dg', word)\n",
    "        return normalized\n",
    "    if val >= 0 and val<1200:\n",
    "        return str(val)\n",
    "    else:\n",
    "        return f'DIGLEN_{str(len(str(val)))}'\n",
    "\n",
    "#funzione che normalizza i token\n",
    "def normalize(word):\n",
    "    if \"http\" in word or (\".\" in word and \"/\" in word):\n",
    "        return str(\"___URL___\")\n",
    "    if len(word)>26:\n",
    "        return \"__LONG-LONG__\"\n",
    "    digits = digit_norm(word)\n",
    "    if digits != word:\n",
    "        word = digits\n",
    "    if word[0].isupper():\n",
    "        word = word.capitalize()\n",
    "    else:\n",
    "        word = word.lower()\n",
    "    return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Le', 'lemma': 'il', 'pos': 'DET'}, {'word': '5', 'lemma': '5', 'pos': 'NUM'}, {'word': 'sgradevoli', 'lemma': 'sgradevole', 'pos': 'ADJ'}, {'word': 'realtà', 'lemma': 'realtà', 'pos': 'NOUN'}, {'word': 'di', 'lemma': 'di', 'pos': 'ADP'}, {'word': 'cui', 'lemma': 'cui', 'pos': 'PRON'}, {'word': 'Berlusconi', 'lemma': 'Berlusconi', 'pos': 'PROPN'}, {'word': 'dovrebbe', 'lemma': 'dovere', 'pos': 'AUX'}, {'word': 'rendersi', 'lemma': 'rendere', 'pos': 'VERB'}, {'word': 'personalmente', 'lemma': 'personalmente', 'pos': 'ADV'}, {'word': 'conto', 'lemma': 'conto', 'pos': 'NOUN'}, {'word': '<url>', 'lemma': '<url>', 'pos': 'PROPN'}, {'word': 'Mario', 'lemma': 'Mario', 'pos': 'PROPN'}, {'word': 'Monti', 'lemma': 'Monti', 'pos': 'PROPN'}, {'word': 'non', 'lemma': 'non', 'pos': 'ADV'}, {'word': 'usa', 'lemma': 'usare', 'pos': 'VERB'}, {'word': 'mezzi', 'lemma': 'mezzo', 'pos': 'ADJ'}, {'word': 'termini', 'lemma': 'termine', 'pos': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "#funzione che estrae dai file conllu i token e li normalizza\n",
    "def get_tokens(doc_path):\n",
    "    doc_tokens = []\n",
    "    skip_lines = 0\n",
    "    first = False\n",
    "    for line in open(doc_path, \"r\", encoding=\"utf-8\"):\n",
    "        splitted = line.strip().split(\"\\t\")\n",
    "        if line[0].isdigit():\n",
    "            if skip_lines == 0 and first == False:\n",
    "                if \"-\" not in splitted[0]:\n",
    "                    word = normalize(splitted[1])\n",
    "                    lemma=normalize(splitted[2])\n",
    "                    pos=splitted[3]\n",
    "                    token = {'word': word, 'lemma': lemma, 'pos': pos}\n",
    "                    doc_tokens.append(token)\n",
    "                else:\n",
    "                    word=normalize(splitted[1])\n",
    "                    skip_lines = len(splitted[0].split('-'))\n",
    "                    first = True\n",
    "            elif skip_lines != 0 and first == True:\n",
    "                lemma = normalize(splitted[2])\n",
    "                pos = splitted[3]\n",
    "                token = {'word': word, 'lemma': lemma, 'pos': pos}\n",
    "                doc_tokens.append(token)\n",
    "                skip_lines-=1\n",
    "                first = False\n",
    "            else: \n",
    "                skip_lines-=1\n",
    "    return doc_tokens\n",
    "\n",
    "#funzione che crea il traingset o test set in base all'argomento specificato\n",
    "def create_set(type):\n",
    "    annotated_posts = []\n",
    "    for doc in os.listdir(\"../data/UD_annotation\"):\n",
    "        if type in doc: \n",
    "            doc_path = \"../data/UD_annotation/\" + doc    \n",
    "            doc_tokens = get_tokens(doc_path)\n",
    "            annotated_posts.append(doc_tokens)\n",
    "    return annotated_posts\n",
    "\n",
    "\n",
    "print(get_tokens(\"../data/UD_annotation/training#125642756147265536#0#0#TW-SENTIPOLC.conllu\"))\n",
    "\n",
    "training_set = create_set(\"training\")\n",
    "test_set = create_set(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '#la@dg', 'lemma': '#la@dg', 'pos': 'NUM'},\n",
       " {'word': 'ma', 'lemma': 'ma', 'pos': 'CCONJ'},\n",
       " {'word': 'perche', 'lemma': 'perco', 'pos': 'ADV'},\n",
       " {'word': \"'\", 'lemma': \"'\", 'pos': 'PUNCT'},\n",
       " {'word': 'Mario', 'lemma': 'Mario', 'pos': 'PROPN'},\n",
       " {'word': 'Monti', 'lemma': 'Monti', 'pos': 'PROPN'},\n",
       " {'word': 'non', 'lemma': 'non', 'pos': 'ADV'},\n",
       " {'word': 'fa', 'lemma': 'fare', 'pos': 'VERB'},\n",
       " {'word': 'il', 'lemma': 'il', 'pos': 'DET'},\n",
       " {'word': 'premier', 'lemma': 'premier', 'pos': 'NOUN'},\n",
       " {'word': '?', 'lemma': '?', 'pos': 'PUNCT'},\n",
       " {'word': 'Che', 'lemma': 'che', 'pos': 'DET'},\n",
       " {'word': 'persona', 'lemma': 'persona', 'pos': 'NOUN'},\n",
       " {'word': 'competente', 'lemma': 'competente', 'pos': 'ADJ'},\n",
       " {'word': 'e', 'lemma': 'e', 'pos': 'CCONJ'},\n",
       " {'word': 'per', 'lemma': 'per', 'pos': 'ADP'},\n",
       " {'word': 'bene', 'lemma': 'bene', 'pos': 'ADV'},\n",
       " {'word': '!', 'lemma': '!', 'pos': 'PUNCT'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
