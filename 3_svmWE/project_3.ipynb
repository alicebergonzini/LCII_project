{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATORE SVM LINEARE CON INPUT DI WORD EMBEDDINGS\n",
    "\n",
    "Classificatore basato su svm lineare che prende in input i word embeddings di parole italiane, più specificamente dei word embedding sviluppati per EvalIta 2018 dall'Italian NLP Lab. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento Word Embeddings\n",
    "\n",
    "Il file txt dei word embedding è stato ottenuto attraverso l'elaborazione di un file sqlite. Per ogni riga (la quale rappresenta una parola) all'interno del file, si crea un record in un dizionario con chiave parola e come valore il vattore che rappresenta il word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per il caricamento degli embedding itwac\n",
    "def load_embeddings():\n",
    "    embeddings_dict = dict()\n",
    "    for line in open('../data/embeddings/itwac32.txt', encoding='utf-8'):\n",
    "        splitted = line.strip().split('\\t')\n",
    "        word = splitted[0]\n",
    "        embedding = splitted[1:]\n",
    "        embedding = [float(val) for val in embedding]\n",
    "        embeddings_dict[word] = np.asarray(embedding)\n",
    "    return embeddings_dict\n",
    "\n",
    "embeddings = load_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02069134,  0.09119736,  0.25785723, -0.23561105, -0.28197852,\n",
       "       -0.13193955, -0.13197723, -0.05229089, -0.28881341,  0.06564969,\n",
       "       -0.30802506,  0.11779311, -0.03571652, -0.08748714, -0.24729131,\n",
       "        0.2577146 ,  0.11925782, -0.27795964,  0.20498367,  0.08414506,\n",
       "        0.08175091, -0.05181665, -0.34403449, -0.05261306,  0.08858071,\n",
       "       -0.09748928,  0.12879393,  0.04387102, -0.04690454,  0.08181785,\n",
       "        0.321078  ,  0.01658573])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['amico']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizzazione del testo \n",
    "\n",
    "Le parole all'interno dei post devono essere normalizzate in questo modo:\n",
    "\n",
    "Numeri:\n",
    "- interi tra 0 e 2100 possono essere mantenuti così come sono\n",
    "- i numeri interi maggiori di 2100 diventano una stringa specifica con un numero che rappresenta il numero di cifre\n",
    "- se non si tratta di numeri interi, si convertono le cifre all'interno della stringa in questa sequenze @Dg\n",
    "\n",
    "Parole:\n",
    "- le parole che iniziano con una lettera maiuscola devono avere prima parola maiuscola e le altre minuscole\n",
    "- le parole che iniziano con una lettera minuscola devono avere tutti i caratteri minuscolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione che trasforma le cifre all'interno di un token\n",
    "def digit_norm(word):\n",
    "    try:\n",
    "        val = int(word)\n",
    "    except:\n",
    "        normalized = re.sub('\\d', '@Dg', word)\n",
    "        return normalized\n",
    "    if val >= 0 and val<1200:\n",
    "        return str(val)\n",
    "    else:\n",
    "        return f'DIGLEN_{str(len(str(val)))}'\n",
    "\n",
    "#funzione che normalizza i token\n",
    "def normalize(word):\n",
    "    if \"http\" in word or (\".\" in word and \"/\" in word):\n",
    "        return str(\"___URL___\")\n",
    "    if len(word)>26:\n",
    "        return \"__LONG-LONG__\"\n",
    "    digits = digit_norm(word)\n",
    "    if digits != word:\n",
    "        word = digits\n",
    "    if word[0].isupper():\n",
    "        word = word.capitalize()\n",
    "    else:\n",
    "        word = word.lower()\n",
    "    return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Le', 'pos': 'DET'}, {'word': '5', 'pos': 'NUM'}, {'word': 'sgradevoli', 'pos': 'ADJ'}, {'word': 'realtà', 'pos': 'NOUN'}, {'word': 'di', 'pos': 'ADP'}, {'word': 'cui', 'pos': 'PRON'}, {'word': 'Berlusconi', 'pos': 'PROPN'}, {'word': 'dovrebbe', 'pos': 'AUX'}, {'word': 'rendersi', 'pos': 'VERB'}, {'word': 'personalmente', 'pos': 'ADV'}, {'word': 'conto', 'pos': 'NOUN'}, {'word': '<url>', 'pos': 'PROPN'}, {'word': 'Mario', 'pos': 'PROPN'}, {'word': 'Monti', 'pos': 'PROPN'}, {'word': 'non', 'pos': 'ADV'}, {'word': 'usa', 'pos': 'VERB'}, {'word': 'mezzi', 'pos': 'ADJ'}, {'word': 'termini', 'pos': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "#funzione che estrae dai file conllu i token e li normalizza\n",
    "def get_tokens(doc_path):\n",
    "    doc_tokens = []\n",
    "    skip_lines = 0\n",
    "    first = False\n",
    "    for line in open(doc_path, \"r\", encoding=\"utf-8\"):\n",
    "        splitted = line.strip().split(\"\\t\")\n",
    "        if line[0].isdigit():\n",
    "            if skip_lines == 0 and first == False:\n",
    "                if \"-\" not in splitted[0]:\n",
    "                    word = normalize(splitted[1])\n",
    "                    pos=splitted[3]\n",
    "                    token = {'word': word, 'pos': pos}\n",
    "                    doc_tokens.append(token)\n",
    "                else:\n",
    "                    word=normalize(splitted[1])\n",
    "                    skip_lines = len(splitted[0].split('-'))\n",
    "                    first = True\n",
    "            elif skip_lines != 0 and first == True:\n",
    "                pos = splitted[3]\n",
    "                token = {'word': word, 'pos': pos}\n",
    "                doc_tokens.append(token)\n",
    "                skip_lines-=1\n",
    "                first = False\n",
    "            else: \n",
    "                skip_lines-=1\n",
    "    return doc_tokens\n",
    "\n",
    "#funzione che crea il traingset o test set in base all'argomento specificato\n",
    "def create_set(type):\n",
    "    annotated_posts = []\n",
    "    for doc in os.listdir(\"../data/UD_annotation\"):\n",
    "        if type in doc: \n",
    "            doc_path = \"../data/UD_annotation/\" + doc    \n",
    "            doc_tokens = get_tokens(doc_path)\n",
    "            annotated_posts.append(doc_tokens)\n",
    "    return annotated_posts\n",
    "\n",
    "\n",
    "print(get_tokens(\"../data/UD_annotation/training#125642756147265536#0#0#TW-SENTIPOLC.conllu\"))\n",
    "\n",
    "training_set = create_set(\"training\")\n",
    "test_set = create_set(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '#la@dg', 'pos': 'NUM'},\n",
       " {'word': 'ma', 'pos': 'CCONJ'},\n",
       " {'word': 'perche', 'pos': 'ADV'},\n",
       " {'word': \"'\", 'pos': 'PUNCT'},\n",
       " {'word': 'Mario', 'pos': 'PROPN'},\n",
       " {'word': 'Monti', 'pos': 'PROPN'},\n",
       " {'word': 'non', 'pos': 'ADV'},\n",
       " {'word': 'fa', 'pos': 'VERB'},\n",
       " {'word': 'il', 'pos': 'DET'},\n",
       " {'word': 'premier', 'pos': 'NOUN'},\n",
       " {'word': '?', 'pos': 'PUNCT'},\n",
       " {'word': 'Che', 'pos': 'DET'},\n",
       " {'word': 'persona', 'pos': 'NOUN'},\n",
       " {'word': 'competente', 'pos': 'ADJ'},\n",
       " {'word': 'e', 'pos': 'CCONJ'},\n",
       " {'word': 'per', 'pos': 'ADP'},\n",
       " {'word': 'bene', 'pos': 'ADV'},\n",
       " {'word': '!', 'pos': 'PUNCT'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Per fare in modo di avere un vettore di features per ogni sample del nostro dataset, per ogni post vanno estratti gli embedding e aggregati secondo una delle seguenti strategie:\n",
    "- Somma \n",
    "- Media \n",
    "- Prodotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Media: [-0.04242259 -0.03806086  0.04385353  0.03976631 -0.12742217  0.10998518\n",
      "  0.0311     -0.06618695  0.06613407 -0.11457642  0.05476216 -0.08994417\n",
      " -0.0447743   0.0523218  -0.17183311  0.0963226  -0.01679959 -0.12544102\n",
      " -0.04104184 -0.09150524 -0.03717459  0.13664455 -0.19373289 -0.04618941\n",
      "  0.06867969  0.0071115   0.02087701  0.03068707 -0.00392625 -0.07004991\n",
      "  0.07070978  0.07404406]\n",
      "Somma: [-0.21211296 -0.19030431  0.21926763  0.19883155 -0.63711086  0.54992588\n",
      "  0.15549998 -0.33093473  0.33067035 -0.57288209  0.27381082 -0.44972083\n",
      " -0.22387151  0.261609   -0.85916553  0.481613   -0.08399795 -0.62720508\n",
      " -0.20520921 -0.4575262  -0.18587295  0.68322273 -0.96866444 -0.23094707\n",
      "  0.34339846  0.0355575   0.10438506  0.15343533 -0.01963124 -0.35024957\n",
      "  0.35354888  0.37022028]\n",
      "Prodotto: [ 1.22333096e-05  1.83019025e-04  2.28736344e-05 -6.08206365e-07\n",
      "  8.52695552e-05 -6.25187651e-05  1.77493883e-05  5.35576757e-06\n",
      " -3.33440727e-05 -6.89323128e-07  5.87005337e-04 -7.91464115e-08\n",
      " -1.44311413e-07  4.67214500e-06  1.28353975e-05 -2.37493070e-05\n",
      " -1.44212375e-06  3.16208824e-05 -6.67220758e-06  1.19991386e-04\n",
      " -1.92214536e-07  1.61567224e-05 -2.34458231e-04  5.04520806e-07\n",
      "  1.15414797e-04 -1.00226486e-04  5.89156950e-07 -2.51684271e-07\n",
      "  4.74331599e-06 -1.34948604e-07  2.96996359e-06  1.00377438e-05]\n"
     ]
    }
   ],
   "source": [
    "#funzione che restituisce gli embedding all'interno di un post\n",
    "def get_post_embeddings(post, consider_pos=False):\n",
    "    post_emb = []\n",
    "    for token in post:\n",
    "        word = token['word']\n",
    "        pos = token['pos']\n",
    "        if not consider_pos: \n",
    "            if word in embeddings:\n",
    "                single_embedding = embeddings[word]\n",
    "                post_emb.append(single_embedding)\n",
    "        else:\n",
    "            if word in embeddings and pos in ['ADJ', 'NOUN', 'VERB']:\n",
    "                single_embedding = embeddings[word]\n",
    "                post_emb.append(single_embedding)\n",
    "    print(len(post_emb))\n",
    "    if len(post_emb)==0:\n",
    "        post_emb = [np.zeros(32)]\n",
    "    return post_emb\n",
    "\n",
    "def post_embeddings_sum(post_emb):\n",
    "    return np.sum(post_emb, axis=0)\n",
    "\n",
    "def post_embeddings_mean(post_emb):\n",
    "    embeddings_sum = post_embeddings_sum(post_emb)\n",
    "    return np.divide(embeddings_sum, len(post_emb))\n",
    "\n",
    "def post_embeddings_prod(post_emb):\n",
    "    return np.prod(post_emb, axis=0)\n",
    "\n",
    "sample = get_post_embeddings(training_set[1], True)\n",
    "\n",
    "print(f'Media: {post_embeddings_mean(sample)}')\n",
    "print(f'Somma: {post_embeddings_sum(sample)}')\n",
    "print(f'Prodotto: {post_embeddings_prod(sample)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
